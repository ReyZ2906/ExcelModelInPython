#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pathlib
import logging
import zipfile
import tempfile
import shutil
from typing import Dict, List, Optional

import pandas as pd
import openpyxl
from openpyxl.styles import PatternFill, Border, Side, Font, Alignment
from openpyxl.utils import get_column_letter
from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE

# -------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)-8s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Paths
# -------------------------------------------------------------------
PROJECT_ROOT = pathlib.Path(__file__).resolve().parents[2]
DATA_DIR = PROJECT_ROOT / "data"
CSV_DIR = PROJECT_ROOT / "csv"
DEAL_ID_XLSX = PROJECT_ROOT / "data" / "deal_id_list.xlsx"
OUTPUT_XLSX = PROJECT_ROOT / "validation_results.xlsx"

CSV_DIR.mkdir(exist_ok=True)


# -------------------------------------------------------------------
# File type helpers
# -------------------------------------------------------------------
def _is_zip_file(p: pathlib.Path) -> bool:
    try:
        return p.read_bytes()[:2] == b"PK"
    except Exception:
        return False


def _is_text_file(p: pathlib.Path) -> bool:
    try:
        p.read_bytes()[:1024].decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception:
        return False


def _read_excel_as_text(path: pathlib.Path) -> pd.DataFrame:
    """Read .xls/.xlsx/.xlsm or pipe-text as DataFrame of strings."""
    suffix = path.suffix.lower()

    if suffix in (".xlsx", ".xlsm") or _is_zip_file(path):
        try:
            df = pd.read_excel(path, dtype=str, engine="openpyxl")
        except Exception as exc:
            raise ValueError(f"Failed to read modern Excel file {path}: {exc}")
        return df.astype(str)

    if suffix == ".xls":
        try:
            df = pd.read_excel(path, dtype=str, engine="xlrd")
        except Exception as exc:
            raise ValueError(f"Failed to read legacy xls file {path}: {exc}")
        return df.astype(str)

    if _is_text_file(path):
        return pd.read_csv(path, sep="|", dtype=str, engine="python")

    raise ValueError(
        f"Cannot determine format of '{path}'. "
        "Supported: .xlsx, .xlsm, .xls, or a pipe-delimited text file."
    )


def _convert_to_csv(src_path: pathlib.Path) -> Optional[pathlib.Path]:
    """Convert Excel/TXT to pipe-delimited CSV under CSV_DIR."""
    csv_path = CSV_DIR / (src_path.stem + ".csv")

    # Already a text file → treat as CSV (pipe-delimited)
    if _is_text_file(src_path):
        if not csv_path.is_file():
            src_path.rename(csv_path)
            log.info(f"Renamed text file to CSV: {src_path.name} -> {csv_path.name}")
        else:
            log.info(f"CSV already exists, skipping rename: {csv_path.name}")
        return csv_path

    # Existing CSV
    if csv_path.is_file():
        return csv_path

    try:
        df = _read_excel_as_text(src_path)
        df.to_csv(csv_path, sep="|", index=False, header=True)
        log.info(f"Converted {src_path.name} -> {csv_path.name}")
    except Exception as exc:
        log.error(f"Failed to convert {src_path.name} to CSV: {exc}")
        return None

    return csv_path


# -------------------------------------------------------------------
# Normalisation helpers
# -------------------------------------------------------------------
def _normalise_column_name(name: str) -> str:
    return name.replace("\u00a0", " ").replace("\u202f", " ").strip()


def _normalise_status_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure there is a canonical 'AUTOMATION STATUS' column based on any
    'automation ... status' column in the file. We do NOT change its
    meaning, only normalise name + type.
    """
    candidates = [c for c in df.columns if "automation" in c.lower() and "status" in c.lower()]
    if not candidates:
        df["AUTOMATION STATUS"] = ""
    else:
        if candidates[0] != "AUTOMATION STATUS":
            df.rename(columns={candidates[0]: "AUTOMATION STATUS"}, inplace=True)

    df["AUTOMATION STATUS"] = df["AUTOMATION STATUS"].fillna("").astype(str)
    return df


def _clean_excel_string(value):
    """Strip characters that Excel/openpyxl cannot handle in worksheet XML."""
    if not isinstance(value, str):
        return value
    return ILLEGAL_CHARACTERS_RE.sub("", value)


def _sanitize_dataframe_for_excel(df: pd.DataFrame) -> pd.DataFrame:
    """Apply Excel-safe cleaning to every cell so to_excel() cannot blow up."""
    if df is None or df.empty:
        return df
    return df.applymap(_clean_excel_string)


def _mark_script_status(df: pd.DataFrame) -> pd.DataFrame:
    """
    Mark rows that look problematic for reading:
    - any cell starting with '=' (formula-like)
    Sets SCRIPT STATUS = 'Read Error' for those rows.
    """
    if df is None or df.empty:
        return df

    if "SCRIPT STATUS" not in df.columns:
        df["SCRIPT STATUS"] = ""

    # Row where ANY cell looks like a formula
    formula_mask = df.applymap(
        lambda v: isinstance(v, str) and v.startswith("=")
    ).any(axis=1)

    df.loc[formula_mask, "SCRIPT STATUS"] = "Read Error"
    return df


# -------------------------------------------------------------------
# CSV loading
# -------------------------------------------------------------------
def _load_csv(path: pathlib.Path, source_label: str) -> pd.DataFrame:
    """
    Load a single CSV, normalising:
      - CORTEX DEPOSIT DEAL ID
      - AUTOMATION STATUS (name + dtype)
      - SCRIPT STATUS (Read Error on formulas)
    If loading fails completely, returns a tiny 'Read Error' row instead
    of killing the pipeline.
    """
    id_col = "CORTEX DEPOSIT DEAL ID"

    try:
        df = pd.read_csv(
            path,
            sep="|",
            dtype=str,
            engine="python",
            on_bad_lines="warn",  # don't crash on broken lines
        )
        df["Report"] = source_label

        if id_col not in df.columns:
            raise KeyError(
                f"The column '{id_col}' is missing from CSV {path.name}. "
                f"Available columns: {list(df.columns)}"
            )

        df[id_col] = (
            df[id_col]
            .astype(str)
            .str.replace("\u00a0", " ", regex=False)
            .str.replace("\u202f", " ", regex=False)
            .str.strip()
            .str.upper()
        )

        df = _normalise_status_column(df)
        df = _mark_script_status(df)

        return df

    except Exception as exc:
        log.error(f"Failed to load CSV {path.name}: {exc}")

        # Represent this file as a single 'Read Error' row
        return pd.DataFrame(
            [{
                id_col: f"READ_ERROR::{path.name}",
                "AUTOMATION STATUS": "",
                "SCRIPT STATUS": "Read Error",
                "Report": source_label,
            }]
        )


# -------------------------------------------------------------------
# Styling helpers
# -------------------------------------------------------------------
def _style_worksheet(ws, status_col_idx: Optional[int] = None) -> None:
    thin = Side(border_style="thin", color="000000")
    border = Border(left=thin, right=thin, top=thin, bottom=thin)

    # Borders
    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
        for cell in row:
            cell.border = border

    # Header styling
    header_fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF")
    header_alignment = Alignment(horizontal="center", vertical="center")

    for cell in ws[1]:
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = header_alignment

    # Status coloring (Success/Failure/Read Error)
    if status_col_idx is not None:
        status_letter = get_column_letter(status_col_idx)
        success_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
        failure_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
        read_error_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

        for cell in ws[status_letter][1:]:
            val = str(cell.value).strip().lower()
            if val == "success":
                cell.fill = success_fill
            elif val in ("failure", "failed"):
                cell.fill = failure_fill
            elif val == "read error":
                cell.fill = read_error_fill

    # Auto column widths
    for col in ws.columns:
        max_length = 0
        column = col[0].column_letter
        for cell in col:
            if cell.value is None:
                continue
            length = len(str(cell.value))
            if length > max_length:
                max_length = length
        ws.column_dimensions[column].width = max_length + 2


def _get_status_column_index(ws, header_name: str) -> Optional[int]:
    for idx, cell in enumerate(ws[1], start=1):
        if str(cell.value).strip().lower() == header_name.strip().lower():
            return idx
    return None


def _write_and_style(
    df: pd.DataFrame,
    writer: pd.ExcelWriter,
    sheet_name: str,
    status_header: str,
) -> None:
    df.to_excel(writer, index=False, sheet_name=sheet_name)
    ws = writer.sheets[sheet_name]
    status_idx = _get_status_column_index(ws, status_header)
    _style_worksheet(ws, status_col_idx=status_idx)


# -------------------------------------------------------------------
# Report discovery
# -------------------------------------------------------------------
def find_all_reports() -> Dict[str, List[pathlib.Path]]:
    raw_patterns = {
        "success": "FTD_SUCC_DEALS_1_*.xls*",
        "failure": "FTD_FAIL_DEALS_1_*.xls*",
    }
    zip_patterns = {
        "success": "Success_*.zip",
        "failure": "Failure_*.zip",
    }

    csv_paths: Dict[str, List[pathlib.Path]] = {"success": [], "failure": []}
    raw_found = False

    # 1) Look for raw Excel in DATA_DIR
    for kind, pat in raw_patterns.items():
        raw_matches = sorted(DATA_DIR.glob(pat))
        if raw_matches:
            raw_found = True
            csv_paths[kind] = [
                _convert_to_csv(p) for p in raw_matches if _convert_to_csv(p) is not None
            ]

    # 2) If no raw Excel, look for ZIPs
    if not raw_found:
        for kind, pat in zip_patterns.items():
            zip_matches = sorted(DATA_DIR.glob(pat))
            if not zip_matches:
                continue
            for zpath in zip_matches:
                with tempfile.TemporaryDirectory() as tmpdir:
                    tmp_dir = pathlib.Path(tmpdir)
                    with zipfile.ZipFile(zpath, "r") as zf:
                        zf.extractall(tmp_dir)

                    for ef in tmp_dir.rglob("*"):
                        if ef.suffix.lower() in (".xls", ".xlsx", ".xlsm", ".txt"):
                            dest = DATA_DIR / ef.name
                            shutil.move(str(ef), dest)
                            converted = _convert_to_csv(dest)
                            if converted is not None:
                                csv_paths[kind].append(converted)

    # 3) If we have any CSV now, return
    if any(csv_paths.values()):
        return csv_paths

    # 4) Fallback: direct CSVs in CSV_DIR
    csv_patterns = {
        "success": "FTD_SUCC_DEALS_1_*.csv",
        "failure": "FTD_FAIL_DEALS_1_*.csv",
    }
    for kind, pat in csv_patterns.items():
        matches = sorted(CSV_DIR.glob(pat))
        csv_paths[kind] = matches

    if not any(csv_paths.values()):
        raise FileNotFoundError(f"No CSV file matching expected patterns in {CSV_DIR}")

    return csv_paths


# -------------------------------------------------------------------
# Deal ID loading
# -------------------------------------------------------------------
def _load_deal_ids() -> List[str]:
    DEAL_ID_SHEET = "DealIDs"
    DEAL_ID_COLUMN = "CORTEX DEPOSIT DEAL ID"

    if not DEAL_ID_XLSX.is_file():
        raise FileNotFoundError(f"Deal-ID workbook not found: {DEAL_ID_XLSX}")

    df = pd.read_excel(
        DEAL_ID_XLSX,
        sheet_name=DEAL_ID_SHEET,
        dtype=str,
        engine="openpyxl",
    )
    df.columns = [_normalise_column_name(col) for col in df.columns]

    if DEAL_ID_COLUMN not in df.columns:
        raise KeyError(
            f"Column '{DEAL_ID_COLUMN}' not found after normalisation. "
            f"Available columns: {list(df.columns)}"
        )

    return (
        df[DEAL_ID_COLUMN]
        .dropna()
        .astype(str)
        .str.strip()
        .str.replace("\u00a0", " ", regex=False)
        .str.replace("\u202f", " ", regex=False)
        .str.upper()
        .tolist()
    )


# -------------------------------------------------------------------
# Result frame building
# -------------------------------------------------------------------
def _build_result_frame(
    deal_ids: List[str],
    combined: pd.DataFrame,
    id_col: str = "CORTEX DEPOSIT DEAL ID",
) -> pd.DataFrame:
    """
    Build a consolidated DataFrame with:
      - Deals-To-Check
      - Status (present in any report → Success, else Failure)
      - SCRIPT STATUS (second status column, may be 'Read Error')
      - All original columns except 'Report'
    """
    if combined is None:
        combined = pd.DataFrame()

    # Ensure SCRIPT STATUS exists to become the "second status" column
    if "SCRIPT STATUS" not in combined.columns:
        combined["SCRIPT STATUS"] = ""

    original_cols = [c for c in combined.columns if c != "Report"]

    rows: List[dict] = []

    for did in deal_ids:
        match = combined[combined[id_col] == did]

        if not match.empty:
            row_dict = match.iloc[0].to_dict()
            row_dict.pop("Report", None)
            status = "Success"
        else:
            row_dict = {c: "" for c in original_cols}
            status = "Failure"

        rows.append(
            {
                "Deals-To-Check": did,
                "Status": status,          # primary status
                **row_dict,                # includes SCRIPT STATUS as second status column
            }
        )

    # We keep columns in a predictable order:
    # Deals-To-Check, Status, SCRIPT STATUS, then the rest
    final_cols = ["Deals-To-Check", "Status"]
    if "SCRIPT STATUS" in original_cols:
        final_cols.append("SCRIPT STATUS")
        original_cols = [c for c in original_cols if c != "SCRIPT STATUS"]
    final_cols.extend(original_cols)

    return pd.DataFrame(rows, columns=final_cols)


# -------------------------------------------------------------------
# Main
# -------------------------------------------------------------------
def main() -> None:
    csv_paths = find_all_reports()
    log.info("CSV files that will be processed:")
    for kind, paths in csv_paths.items():
        for p in paths:
            log.info(f"  {kind.title():7} -> {p.name}")

    deal_ids = _load_deal_ids()
    if not deal_ids:
        log.warning("No Deal IDs found - exiting.")
        return

    log.info(f"Number of Deal IDs to validate: {len(deal_ids)}")

    # Build success_df robustly
    success_frames = [_load_csv(p, "Success") for p in csv_paths["success"]]
    success_frames = [df for df in success_frames if df is not None and not df.empty]
    success_df = pd.concat(success_frames, ignore_index=True) if success_frames else pd.DataFrame()

    # Build failure_df robustly
    failure_frames = [_load_csv(p, "Failure") for p in csv_paths["failure"]]
    failure_frames = [df for df in failure_frames if df is not None and not df.empty]
    failure_df = pd.concat(failure_frames, ignore_index=True) if failure_frames else pd.DataFrame()

    # Combine success + failure
    if not success_df.empty and not failure_df.empty:
        combined = pd.concat([success_df, failure_df], ignore_index=True)
    elif not success_df.empty:
        combined = success_df
    elif not failure_df.empty:
        combined = failure_df
    else:
        log.warning("No usable data from success/failure CSVs.")
        combined = pd.DataFrame()

    allmatched_df = _build_result_frame(deal_ids, combined)

    # Filters using AUTMATION STATUS ONLY (file column, not modified semantically)
    if "AUTOMATION STATUS" not in allmatched_df.columns:
        allmatched_df["AUTOMATION STATUS"] = ""

    success_mask = (
        (allmatched_df["Status"].str.upper() == "SUCCESS")
        & (allmatched_df["AUTOMATION STATUS"].str.upper() == "SUCCESS")
    )
    allmatched_success = allmatched_df[success_mask]

    fail_mask = (
        (allmatched_df["Status"].str.upper() == "SUCCESS")
        & (allmatched_df["AUTOMATION STATUS"].str.upper().isin(["FAILED", "FAILURE"]))
    )
    allmatched_fail = allmatched_df[fail_mask]

    # Clean illegal characters so Excel write never crashes
    allmatched_df = _sanitize_dataframe_for_excel(allmatched_df)
    allmatched_success = _sanitize_dataframe_for_excel(allmatched_success)
    allmatched_fail = _sanitize_dataframe_for_excel(allmatched_fail)

    # Write output
    with pd.ExcelWriter(OUTPUT_XLSX, engine="openpyxl") as writer:
        # In AllMatched, SECOND status column is SCRIPT STATUS → Read Error (yellow)
        _write_and_style(
            allmatched_df,
            writer,
            sheet_name="AllMatched",
            status_header="SCRIPT STATUS",
        )

        # In bifurcated sheets, use AUTOMATION STATUS for coloring
        _write_and_style(
            allmatched_success,
            writer,
            sheet_name="AllMatched_Success",
            status_header="AUTOMATION STATUS",
        )
        _write_and_style(
            allmatched_fail,
            writer,
            sheet_name="AllMatched_Fail",
            status_header="AUTOMATION STATUS",
        )

    log.info(f"Results saved to: {OUTPUT_XLSX}")
    if not allmatched_df.empty:
        log.info(f"  → Total rows in AllMatched: {len(allmatched_df)}")
        log.info(f"  → Rows in AllMatched_Success: {len(allmatched_success)}")
        log.info(f"  → Rows in AllMatched_Fail: {len(allmatched_fail)}")
    else:
        log.info("  → No matching Deal IDs found in the reports.")


if __name__ == "__main__":
    main()
