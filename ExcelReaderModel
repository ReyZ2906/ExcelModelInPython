#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pathlib
import logging
import zipfile
import tempfile
import shutil
from typing import Dict, List, Optional

import pandas as pd
from openpyxl.styles import PatternFill, Border, Side, Font, Alignment
from openpyxl.utils import get_column_letter
from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
from openpyxl.utils.exceptions import IllegalCharacterError

# -------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)-8s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Paths
# -------------------------------------------------------------------
PROJECT_ROOT = pathlib.Path(__file__).resolve().parents[2]
DATA_DIR = PROJECT_ROOT / "data"
CSV_DIR = PROJECT_ROOT / "csv"
DEAL_ID_XLSX = PROJECT_ROOT / "data" / "deal_id_list.xlsx"
OUTPUT_XLSX = PROJECT_ROOT / "validation_results.xlsx"

CSV_DIR.mkdir(exist_ok=True)


# -------------------------------------------------------------------
# File type helpers
# -------------------------------------------------------------------
def _is_zip_file(p: pathlib.Path) -> bool:
    try:
        return p.read_bytes()[:2] == b"PK"
    except Exception:
        return False


def _is_text_file(p: pathlib.Path) -> bool:
    try:
        p.read_bytes()[:1024].decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception:
        return False


def _read_excel_as_text(path: pathlib.Path) -> pd.DataFrame:
    suffix = path.suffix.lower()

    if suffix in (".xlsx", ".xlsm") or _is_zip_file(path):
        df = pd.read_excel(path, dtype=str, engine="openpyxl")
        return df.astype(str)

    if suffix == ".xls":
        df = pd.read_excel(path, dtype=str, engine="xlrd")
        return df.astype(str)

    if _is_text_file(path):
        return pd.read_csv(path, sep="|", dtype=str, engine="python")

    raise ValueError(
        f"Cannot determine format of '{path}'. "
        "Supported: .xlsx, .xlsm, .xls, or a pipe-delimited text file."
    )


def _convert_to_csv(src_path: pathlib.Path) -> Optional[pathlib.Path]:
    csv_path = CSV_DIR / (src_path.stem + ".csv")

    if _is_text_file(src_path):
        if not csv_path.is_file():
            src_path.rename(csv_path)
            log.info(f"Renamed text file to CSV: {src_path.name} -> {csv_path.name}")
        else:
            log.info(f"CSV already exists, skipping rename: {csv_path.name}")
        return csv_path

    if csv_path.is_file():
        return csv_path

    try:
        df = _read_excel_as_text(src_path)
        df.to_csv(csv_path, sep="|", index=False, header=True)
        log.info(f"Converted {src_path.name} -> {csv_path.name}")
    except Exception as exc:
        log.error(f"Failed to convert {src_path.name} to CSV: {exc}")
        return None

    return csv_path


# -------------------------------------------------------------------
# Normalisation / cleaning
# -------------------------------------------------------------------
def _normalise_column_name(name: str) -> str:
    return name.replace("\u00a0", " ").replace("\u202f", " ").strip()


def _normalise_status_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure there is an 'AUTOMATION STATUS' column.
    We do NOT change its meaning – only header + dtype.
    """
    candidates = [c for c in df.columns if "automation" in c.lower() and "status" in c.lower()]
    if not candidates:
        df["AUTOMATION STATUS"] = ""
    else:
        if candidates[0] != "AUTOMATION STATUS":
            df.rename(columns={candidates[0]: "AUTOMATION STATUS"}, inplace=True)

    df["AUTOMATION STATUS"] = df["AUTOMATION STATUS"].fillna("").astype(str)
    return df


def _clean_excel_string(value):
    """
    Make a value 100% safe for Excel:
    - remove known illegal chars via openpyxl regex
    - drop any non-printable control characters
    """
    if not isinstance(value, str):
        return value

    # remove XML-illegal control chars
    value = ILLEGAL_CHARACTERS_RE.sub("", value)

    # drop everything that is not a "reasonable" printable char
    safe_chars = []
    for ch in value:
        code = ord(ch)
        # keep tab/newline/carriage-return
        if ch in ("\t", "\n", "\r"):
            safe_chars.append(ch)
        # keep basic printable range
        elif 32 <= code <= 126:
            safe_chars.append(ch)
        # optionally keep extended latin letters if you want:
        elif 160 <= code <= 255:
            safe_chars.append(ch)
        # else: skip
    return "".join(safe_chars)


def _sanitize_dataframe_for_excel(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    return df.applymap(_clean_excel_string)


def _mark_script_status(df: pd.DataFrame) -> pd.DataFrame:
    """
    Second status column: SCRIPT STATUS
    - if any cell in a row looks like a formula (=...), mark as 'Read Error'
    """
    if df is None or df.empty:
        return df

    if "SCRIPT STATUS" not in df.columns:
        df["SCRIPT STATUS"] = ""

    formula_mask = df.applymap(
        lambda v: isinstance(v, str) and v.startswith("=")
    ).any(axis=1)

    df.loc[formula_mask, "SCRIPT STATUS"] = "Read Error"
    return df


# -------------------------------------------------------------------
# CSV loading
# -------------------------------------------------------------------
def _load_csv(path: pathlib.Path, source_label: str) -> pd.DataFrame:
    id_col = "CORTEX DEPOSIT DEAL ID"
    try:
        df = pd.read_csv(
            path,
            sep="|",
            dtype=str,
            engine="python",
            on_bad_lines="warn",  # don't die on malformed lines
        )
        df["Report"] = source_label

        if id_col not in df.columns:
            raise KeyError(
                f"The column '{id_col}' is missing from CSV {path.name}. "
                f"Available columns: {list(df.columns)}"
            )

        df[id_col] = (
            df[id_col]
            .astype(str)
            .str.replace("\u00a0", " ", regex=False)
            .str.replace("\u202f", " ", regex=False)
            .str.strip()
            .str.upper()
        )

        df = _normalise_status_column(df)
        df = _mark_script_status(df)

        return df

    except Exception as exc:
        log.error(f"Failed to load CSV {path.name}: {exc}")
        # represent file as a single Read Error row
        return pd.DataFrame(
            [{
                id_col: f"READ_ERROR::{path.name}",
                "AUTOMATION STATUS": "",
                "SCRIPT STATUS": "Read Error",
                "Report": source_label,
            }]
        )


# -------------------------------------------------------------------
# Styling helpers
# -------------------------------------------------------------------
def _style_worksheet(ws, status_col_idx: Optional[int] = None) -> None:
    thin = Side(border_style="thin", color="000000")
    border = Border(left=thin, right=thin, top=thin, bottom=thin)

    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
        for cell in row:
            cell.border = border

    header_fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF")
    header_alignment = Alignment(horizontal="center", vertical="center")

    for cell in ws[1]:
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = header_alignment

    if status_col_idx is not None:
        status_letter = get_column_letter(status_col_idx)
        success_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
        failure_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
        read_error_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

        for cell in ws[status_letter][1:]:
            val = str(cell.value).strip().lower()
            if val == "success":
                cell.fill = success_fill
            elif val in ("failure", "failed"):
                cell.fill = failure_fill
            elif val == "read error":
                cell.fill = read_error_fill

    for col in ws.columns:
        max_len = 0
        column = col[0].column_letter
        for cell in col:
            if cell.value is None:
                continue
            max_len = max(max_len, len(str(cell.value)))
        ws.column_dimensions[column].width = max_len + 2


def _get_status_column_index(ws, header_name: str) -> Optional[int]:
    for idx, cell in enumerate(ws[1], start=1):
        if str(cell.value).strip().lower() == header_name.strip().lower():
            return idx
    return None


def _write_and_style(
    df: pd.DataFrame,
    writer: pd.ExcelWriter,
    sheet_name: str,
    status_header: str,
) -> None:
    df.to_excel(writer, index=False, sheet_name=sheet_name)
    ws = writer.sheets[sheet_name]
    status_idx = _get_status_column_index(ws, status_header)
    _style_worksheet(ws, status_col_idx=status_idx)


# -------------------------------------------------------------------
# Report discovery
# -------------------------------------------------------------------
def find_all_reports() -> Dict[str, List[pathlib.Path]]:
    raw_patterns = {
        "success": "FTD_SUCC_DEALS_1_*.xls*",
        "failure": "FTD_FAIL_DEALS_1_*.xls*",
    }
    zip_patterns = {
        "success": "Success_*.zip",
        "failure": "Failure_*.zip",
    }

    csv_paths: Dict[str, List[pathlib.Path]] = {"success": [], "failure": []}
    raw_found = False

    for kind, pat in raw_patterns.items():
        raw_matches = sorted(DATA_DIR.glob(pat))
        if raw_matches:
            raw_found = True
            csv_paths[kind] = [
                _convert_to_csv(p) for p in raw_matches if _convert_to_csv(p) is not None
            ]

    if not raw_found:
        for kind, pat in zip_patterns.items():
            zip_matches = sorted(DATA_DIR.glob(pat))
            if not zip_matches:
                continue
            for zpath in zip_matches:
                with tempfile.TemporaryDirectory() as tmpdir:
                    tmp_dir = pathlib.Path(tmpdir)
                    with zipfile.ZipFile(zpath, "r") as zf:
                        zf.extractall(tmp_dir)

                    for ef in tmp_dir.rglob("*"):
                        if ef.suffix.lower() in (".xls", ".xlsx", ".xlsm", ".txt"):
                            dest = DATA_DIR / ef.name
                            shutil.move(str(ef), dest)
                            converted = _convert_to_csv(dest)
                            if converted is not None:
                                csv_paths[kind].append(converted)

    if any(csv_paths.values()):
        return csv_paths

    csv_patterns = {
        "success": "FTD_SUCC_DEALS_1_*.csv",
        "failure": "FTD_FAIL_DEALS_1_*.csv",
    }
    for kind, pat in csv_patterns.items():
        csv_paths[kind] = sorted(CSV_DIR.glob(pat))

    if not any(csv_paths.values()):
        raise FileNotFoundError(f"No CSV file matching expected patterns in {CSV_DIR}")

    return csv_paths


# -------------------------------------------------------------------
# Deal IDs
# -------------------------------------------------------------------
def _load_deal_ids() -> List[str]:
    DEAL_ID_SHEET = "DealIDs"
    DEAL_ID_COLUMN = "CORTEX DEPOSIT DEAL ID"

    if not DEAL_ID_XLSX.is_file():
        raise FileNotFoundError(f"Deal-ID workbook not found: {DEAL_ID_XLSX}")

    df = pd.read_excel(
        DEAL_ID_XLSX,
        sheet_name=DEAL_ID_SHEET,
        dtype=str,
        engine="openpyxl",
    )
    df.columns = [_normalise_column_name(col) for col in df.columns]

    if DEAL_ID_COLUMN not in df.columns:
        raise KeyError(
            f"Column '{DEAL_ID_COLUMN}' not found after normalisation. "
            f"Available columns: {list(df.columns)}"
        )

    return (
        df[DEAL_ID_COLUMN]
        .dropna()
        .astype(str)
        .str.strip()
        .str.replace("\u00a0", " ", regex=False)
        .str.replace("\u202f", " ", regex=False)
        .str.upper()
        .tolist()
    )


# -------------------------------------------------------------------
# Result frame
# -------------------------------------------------------------------
def _build_result_frame(
    deal_ids: List[str],
    combined: pd.DataFrame,
    id_col: str = "CORTEX DEPOSIT DEAL ID",
) -> pd.DataFrame:
    if combined is None:
        combined = pd.DataFrame()

    if "SCRIPT STATUS" not in combined.columns:
        combined["SCRIPT STATUS"] = ""

    original_cols = [c for c in combined.columns if c != "Report"]

    rows: List[dict] = []

    for did in deal_ids:
        match = combined[combined[id_col] == did]

        if not match.empty:
            row_dict = match.iloc[0].to_dict()
            row_dict.pop("Report", None)
            status = "Success"
        else:
            row_dict = {c: "" for c in original_cols}
            status = "Failure"

        rows.append(
            {
                "Deals-To-Check": did,
                "Status": status,
                **row_dict,
            }
        )

    final_cols = ["Deals-To-Check", "Status"]
    if "SCRIPT STATUS" in original_cols:
        final_cols.append("SCRIPT STATUS")
        original_cols = [c for c in original_cols if c != "SCRIPT STATUS"]
    final_cols.extend(original_cols)

    return pd.DataFrame(rows, columns=final_cols)


# -------------------------------------------------------------------
# Main
# -------------------------------------------------------------------
def main() -> None:
    csv_paths = find_all_reports()
    log.info("CSV files that will be processed:")
    for kind, paths in csv_paths.items():
        for p in paths:
            log.info(f"  {kind.title():7} -> {p.name}")

    deal_ids = _load_deal_ids()
    if not deal_ids:
        log.warning("No Deal IDs found - exiting.")
        return

    log.info(f"Number of Deal IDs to validate: {len(deal_ids)}")

    success_frames = [_load_csv(p, "Success") for p in csv_paths["success"]]
    success_frames = [df for df in success_frames if df is not None and not df.empty]
    success_df = pd.concat(success_frames, ignore_index=True) if success_frames else pd.DataFrame()

    failure_frames = [_load_csv(p, "Failure") for p in csv_paths["failure"]]
    failure_frames = [df for df in failure_frames if df is not None and not df.empty]
    failure_df = pd.concat(failure_frames, ignore_index=True) if failure_frames else pd.DataFrame()

    if not success_df.empty and not failure_df.empty:
        combined = pd.concat([success_df, failure_df], ignore_index=True)
    elif not success_df.empty:
        combined = success_df
    elif not failure_df.empty:
        combined = failure_df
    else:
        log.warning("No usable data from success/failure CSVs.")
        combined = pd.DataFrame()

    allmatched_df = _build_result_frame(deal_ids, combined)

    if "AUTOMATION STATUS" not in allmatched_df.columns:
        allmatched_df["AUTOMATION STATUS"] = ""

    success_mask = (
        (allmatched_df["Status"].str.upper() == "SUCCESS")
        & (allmatched_df["AUTOMATION STATUS"].str.upper() == "SUCCESS")
    )
    allmatched_success = allmatched_df[success_mask]

    fail_mask = (
        (allmatched_df["Status"].str.upper() == "SUCCESS")
        & (allmatched_df["AUTOMATION STATUS"].str.upper().isin(["FAILED", "FAILURE"]))
    )
    allmatched_fail = allmatched_df[fail_mask]

    # FINAL sanitise before Excel write
    allmatched_df = _sanitize_dataframe_for_excel(allmatched_df)
    allmatched_success = _sanitize_dataframe_for_excel(allmatched_success)
    allmatched_fail = _sanitize_dataframe_for_excel(allmatched_fail)

    try:
        with pd.ExcelWriter(OUTPUT_XLSX, engine="openpyxl") as writer:
            # AllMatched → color by SCRIPT STATUS (second status column)
            _write_and_style(
                allmatched_df,
                writer,
                sheet_name="AllMatched",
                status_header="SCRIPT STATUS",
            )

            # Bifurcated sheets → color by AUTOMATION STATUS
            _write_and_style(
                allmatched_success,
                writer,
                sheet_name="AllMatched_Success",
                status_header="AUTOMATION STATUS",
            )
            _write_and_style(
                allmatched_fail,
                writer,
                sheet_name="AllMatched_Fail",
                status_header="AUTOMATION STATUS",
            )

        log.info(f"Results saved to: {OUTPUT_XLSX}")
        print(f"Output file generated successfully: {OUTPUT_XLSX}")

    except IllegalCharacterError as exc:
        # If this ever triggers, we know some char still slipped through
        log.error(f"Fatal IllegalCharacterError while writing workbook: {exc}")
        raise


if __name__ == "__main__":
    main()

