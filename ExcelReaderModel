#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pathlib
import logging
import zipfile
import tempfile
import shutil
from typing import Dict, List, Optional

import pandas as pd
from openpyxl.styles import PatternFill, Border, Side, Font, Alignment
from openpyxl.utils import get_column_letter

# -------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)-8s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Paths
# -------------------------------------------------------------------
PROJECT_ROOT = pathlib.Path(__file__).resolve().parents[2]
DATA_DIR = PROJECT_ROOT / "data"
CSV_DIR = PROJECT_ROOT / "csv"
DEAL_ID_XLSX = PROJECT_ROOT / "data" / "deal_id_list.xlsx"
OUTPUT_XLSX = PROJECT_ROOT / "validation_results.xlsx"

CSV_DIR.mkdir(exist_ok=True)

# -------------------------------------------------------------------
# Global flags for read errors (no extra visible columns)
# -------------------------------------------------------------------
READ_ERROR_DEAL_IDS: set[str] = set()
GLOBAL_FILE_READ_ERROR: bool = False


# -------------------------------------------------------------------
# File-type helpers
# -------------------------------------------------------------------
def _is_zip_file(p: pathlib.Path) -> bool:
    try:
        return p.read_bytes()[:2] == b"PK"
    except Exception:
        return False


def _is_text_file(p: pathlib.Path) -> bool:
    try:
        p.read_bytes()[:1024].decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception:
        return False


def _read_excel_as_text(path: pathlib.Path) -> pd.DataFrame:
    """Read xlsx/xlsm/xls or pipe-text as DataFrame of strings."""
    suffix = path.suffix.lower()

    if suffix in (".xlsx", ".xlsm") or _is_zip_file(path):
        df = pd.read_excel(path, dtype=str, engine="openpyxl")
        return df.astype(str)

    if suffix == ".xls":
        df = pd.read_excel(path, dtype=str, engine="xlrd")
        return df.astype(str)

    if _is_text_file(path):
        return pd.read_csv(path, sep="|", dtype=str, engine="python")

    raise ValueError(
        f"Cannot determine format of '{path}'. Supported: xlsx/xlsm/xls/txt(pipe)."
    )


def _convert_to_csv(src_path: pathlib.Path) -> Optional[pathlib.Path]:
    """Convert Excel/TXT to pipe-delimited CSV in CSV_DIR."""
    csv_path = CSV_DIR / (src_path.stem + ".csv")

    if _is_text_file(src_path):
        if not csv_path.exists():
            src_path.rename(csv_path)
            log.info(f"Renamed text to CSV: {src_path.name} -> {csv_path.name}")
        return csv_path

    if csv_path.exists():
        return csv_path

    try:
        df = _read_excel_as_text(src_path)
        df.to_csv(csv_path, sep="|", index=False)
        log.info(f"Converted {src_path.name} → {csv_path.name}")
    except Exception as exc:
        log.error(f"Failed to convert {src_path.name}: {exc}")
        return None

    return csv_path


# -------------------------------------------------------------------
# Cleaning helpers (strict XML-safe for Excel)
# -------------------------------------------------------------------
def _clean_excel_string(value):
    """Keep only XML 1.0 legal chars so openpyxl never crashes."""
    if not isinstance(value, str):
        return value

    def allowed(ch):
        cp = ord(ch)
        return (
            cp == 0x9
            or cp == 0xA
            or cp == 0xD
            or 0x20 <= cp <= 0xD7FF
            or 0xE000 <= cp <= 0xFFFD
            or 0x10000 <= cp <= 0x10FFFF
        )

    return "".join(ch for ch in value if allowed(ch))


def _sanitize_dataframe_for_excel(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    return df.applymap(_clean_excel_string)


# -------------------------------------------------------------------
# Normalisation
# -------------------------------------------------------------------
def _normalise_column_name(name: str) -> str:
    return name.replace("\u00a0", " ").replace("\u202f", " ").strip()


def _normalise_status_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure there is an 'AUTOMATION STATUS' column.
    We DO NOT change its meaning, only normalise header + dtype.
    """
    candidates = [c for c in df.columns if "automation" in c.lower() and "status" in c.lower()]

    if not candidates:
        df["AUTOMATION STATUS"] = ""
    else:
        if candidates[0] != "AUTOMATION STATUS":
            df.rename(columns={candidates[0]: "AUTOMATION STATUS"}, inplace=True)

    df["AUTOMATION STATUS"] = df["AUTOMATION STATUS"].fillna("").astype(str)
    return df


# -------------------------------------------------------------------
# CSV loading (does NOT add Status column here)
# -------------------------------------------------------------------
def _load_csv(path: pathlib.Path, source_label: str) -> pd.DataFrame:
    """
    Load CSV from report. No structural changes, only:
      - normalise CORTEX DEPOSIT DEAL ID
      - normalise AUTMATION STATUS header
      - record read-error deal IDs (for formulas) in a global set
      - record file-level read failures in GLOBAL_FILE_READ_ERROR
    """
    global READ_ERROR_DEAL_IDS, GLOBAL_FILE_READ_ERROR

    id_col = "CORTEX DEPOSIT DEAL ID"

    try:
        df = pd.read_csv(
            path,
            sep="|",
            dtype=str,
            engine="python",
            on_bad_lines="warn",  # don't die on bad lines
        )
        df["Report"] = source_label

        if id_col not in df.columns:
            raise KeyError(f"{id_col} missing in {path.name}")

        df[id_col] = (
            df[id_col]
            .astype(str)
            .str.replace("\u00a0", " ", regex=False)
            .str.replace("\u202f", " ", regex=False)
            .str.strip()
            .str.upper()
        )

        df = _normalise_status_column(df)

        # Detect formula-like content anywhere in the row
        formula_mask = df.applymap(
            lambda v: isinstance(v, str) and v.startswith("=")
        ).any(axis=1)
        if formula_mask.any():
            deals_with_formula = (
                df.loc[formula_mask, id_col]
                .dropna()
                .astype(str)
                .str.strip()
                .str.upper()
                .tolist()
            )
            READ_ERROR_DEAL_IDS.update(deals_with_formula)

        return df

    except Exception as exc:
        log.error(f"CSV read failure {path.name}: {exc}")
        GLOBAL_FILE_READ_ERROR = True
        # return empty DF; Status will be decided later
        return pd.DataFrame()


# -------------------------------------------------------------------
# Styling (same for Status and Automation Status, based on values)
# -------------------------------------------------------------------
def _style_worksheet(ws, status_col_idx: Optional[int]):
    thin = Side(border_style="thin", color="000000")
    border = Border(left=thin, right=thin, top=thin, bottom=thin)

    # Borders
    for row in ws.iter_rows():
        for cell in row:
            cell.border = border

    # Header
    header_fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF")
    header_alignment = Alignment(horizontal="center", vertical="center")

    for cell in ws[1]:
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = header_alignment

    # Status-based colours
    if status_col_idx:
        col_letter = get_column_letter(status_col_idx)

        green = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
        red = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
        yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        red_font = Font(color="FF0000")

        for cell in ws[col_letter][1:]:
            v = str(cell.value).strip().lower()

            # Treat "success" OR "found" as green
            if v in ("success", "found"):
                cell.fill = green

            # Treat "failure"/"failed" OR "not found" as red
            elif v in ("failure", "failed", "not found"):
                cell.fill = red

            # Read Error → yellow + red text
            elif v == "read error":
                cell.fill = yellow
                cell.font = red_font

    # Auto column width
    for col in ws.columns:
        length = max(len(str(c.value) if c.value is not None else "") for c in col)
        ws.column_dimensions[col[0].column_letter].width = length + 2


def _get_status_col(ws, header_name: str) -> Optional[int]:
    for idx, cell in enumerate(ws[1], start=1):
        if str(cell.value).strip().lower() == header_name.strip().lower():
            return idx
    return None


def _write_and_style(df: pd.DataFrame, writer: pd.ExcelWriter, sheet_name: str, status_header: str):
    df.to_excel(writer, index=False, sheet_name=sheet_name)
    ws = writer.sheets[sheet_name]
    status_idx = _get_status_col(ws, status_header)
    _style_worksheet(ws, status_idx)


# -------------------------------------------------------------------
# Report discovery
# -------------------------------------------------------------------
def find_all_reports() -> Dict[str, List[pathlib.Path]]:
    raw_patterns = {
        "success": "FTD_SUCC_DEALS_1_*.xls*",
        "failure": "FTD_FAIL_DEALS_1_*.xls*",
    }

    csv_paths: Dict[str, List[pathlib.Path]] = {"success": [], "failure": []}
    raw_found = False

    # 1) Try raw Excel
    for kind, pat in raw_patterns.items():
        matches = list(DATA_DIR.glob(pat))
        if matches:
            raw_found = True
            for m in matches:
                c = _convert_to_csv(m)
                if c:
                    csv_paths[kind].append(c)

    # 2) If none, try ZIPs
    if not raw_found:
        zip_patterns = {
            "success": "Success_*.zip",
            "failure": "Failure_*.zip",
        }
        for kind, pat in zip_patterns.items():
            for z in DATA_DIR.glob(pat):
                with tempfile.TemporaryDirectory() as t:
                    tmp_dir = pathlib.Path(t)
                    with zipfile.ZipFile(z, "r") as zf:
                        zf.extractall(tmp_dir)
                    for f in tmp_dir.rglob("*"):
                        if f.suffix.lower() in (".xls", ".xlsx", ".xlsm", ".txt"):
                            dest = DATA_DIR / f.name
                            shutil.move(str(f), dest)
                            c = _convert_to_csv(dest)
                            if c:
                                csv_paths[kind].append(c)

    # 3) Fallback: direct CSVs
    if not any(csv_paths.values()):
        fallback_patterns = {
            "success": "FTD_SUCC_DEALS_1_*.csv",
            "failure": "FTD_FAIL_DEALS_1_*.csv",
        }
        for kind, pat in fallback_patterns.items():
            csv_paths[kind] = list(CSV_DIR.glob(pat))

    return csv_paths


# -------------------------------------------------------------------
# Deal IDs
# -------------------------------------------------------------------
def _load_deal_ids() -> List[str]:
    df = pd.read_excel(DEAL_ID_XLSX, sheet_name="DealIDs", dtype=str, engine="openpyxl")
    df.columns = [_normalise_column_name(c) for c in df.columns]
    col = "CORTEX DEPOSIT DEAL ID"

    return (
        df[col]
        .dropna()
        .astype(str)
        .str.strip()
        .str.replace("\u00a0", " ", regex=False)
        .str.replace("\u202f", " ", regex=False)
        .str.upper()
        .tolist()
    )


# -------------------------------------------------------------------
# Result frame (Status = Found / Not Found / Read Error)
# -------------------------------------------------------------------
def _build_result_frame(
    deal_ids: List[str],
    combined: pd.DataFrame,
    id_col: str = "CORTEX DEPOSIT DEAL ID",
) -> pd.DataFrame:
    """
    Preserve original structure:
      - "Deals-To-Check"
      - "Status" (now: Found / Not Found / Read Error)
      - then all original columns in the same order (except Report)
    """
    original_cols = [c for c in combined.columns if c != "Report"]
    rows: List[Dict[str, str]] = []

    for did in deal_ids:
        match = combined[combined[id_col] == did]

        if not match.empty:
            row_dict = match.iloc[0].to_dict()
            row_dict.pop("Report", None)
            status = "Found"
        else:
            row_dict = {c: "" for c in original_cols}
            status = "Not Found"

        # Override with Read Error if:
        # - this deal appeared in any formula-affected row, OR
        # - file-level read error happened and the deal wasn't found
        if did in READ_ERROR_DEAL_IDS or (status == "Not Found" and GLOBAL_FILE_READ_ERROR):
            status = "Read Error"

        rows.append(
            {
                "Deals-To-Check": did,
                "Status": status,
                **row_dict,
            }
        )

    final_cols = ["Deals-To-Check", "Status"] + original_cols
    return pd.DataFrame(rows, columns=final_cols)


# -------------------------------------------------------------------
# MAIN
# -------------------------------------------------------------------
def main() -> None:
    csv_paths = find_all_reports()
    log.info("CSV files that will be processed:")
    for kind, paths in csv_paths.items():
        for p in paths:
            log.info(f"  {kind.title():7} -> {p.name}")

    deal_ids = _load_deal_ids()
    if not deal_ids:
        log.warning("No Deal IDs found - exiting.")
        return

    log.info(f"Number of Deal IDs to validate: {len(deal_ids)}")

    # Build combined DF from success/failure CSVs (structure unchanged)
    success_frames = [_load_csv(p, "Success") for p in csv_paths["success"]]
    success_frames = [df for df in success_frames if not df.empty]
    success_df = pd.concat(success_frames, ignore_index=True) if success_frames else pd.DataFrame()

    failure_frames = [_load_csv(p, "Failure") for p in csv_paths["failure"]]
    failure_frames = [df for df in failure_frames if not df.empty]
    failure_df = pd.concat(failure_frames, ignore_index=True) if failure_frames else pd.DataFrame()

    if not success_df.empty and not failure_df.empty:
        combined = pd.concat([success_df, failure_df], ignore_index=True)
    elif not success_df.empty:
        combined = success_df
    else:
        combined = failure_df

    # Build AllMatched frame with new Status semantics
    allmatched_df = _build_result_frame(deal_ids, combined)

    # Ensure AUTMATION STATUS exists (for bifurcation only)
    if "AUTOMATION STATUS" not in allmatched_df.columns:
        allmatched_df["AUTOMATION STATUS"] = ""

    # Clean illegal characters before Excel write
    allmatched_df = _sanitize_dataframe_for_excel(allmatched_df)

    # Bifurcation: **only when Status == Found**
    success_mask = (
        (allmatched_df["Status"].str.lower() == "found")
        & (allmatched_df["AUTOMATION STATUS"].str.upper() == "SUCCESS")
    )
    fail_mask = (
        (allmatched_df["Status"].str.lower() == "found")
        & (allmatched_df["AUTOMATION STATUS"].str.upper().isin(["FAILED", "FAILURE"]))
    )

    allmatched_success = allmatched_df[success_mask]
    allmatched_fail = allmatched_df[fail_mask]

    # Write output
    with pd.ExcelWriter(OUTPUT_XLSX, engine="openpyxl") as writer:
        # AllMatched → styles by Status (Found / Not Found / Read Error)
        _write_and_style(allmatched_df, writer, "AllMatched", status_header="Status")

        # Bifurcated sheets → styles by AUTOMATION STATUS (Success / Failure / Read Error if present)
        _write_and_style(
            allmatched_success,
            writer,
            "AllMatched_Success",
            status_header="AUTOMATION STATUS",
        )
        _write_and_style(
            allmatched_fail,
            writer,
            "AllMatched_Fail",
            status_header="AUTOMATION STATUS",
        )

    log.info(f"Results saved to: {OUTPUT_XLSX}")
    print(f"Output Excel generated: {OUTPUT_XLSX}")


if __name__ == "__main__":
    main()
